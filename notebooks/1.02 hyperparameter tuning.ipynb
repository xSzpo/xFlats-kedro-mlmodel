{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "worth-change",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-05 15:44:44,675 - kedro.framework.session.store - INFO - `read()` not implemented for `BaseSessionStore`. Assuming empty store.\n",
      "2021-04-05 15:44:44,781 - root - INFO - Registered hooks from 2 installed plugin(s): kedro-mlflow-0.7.0\n",
      "2021-04-05 15:44:44,809 - root - INFO - ** Kedro project xflats\n",
      "2021-04-05 15:44:44,810 - root - INFO - Defined global variable `context`, `session` and `catalog`\n",
      "2021-04-05 15:44:44,834 - root - INFO - Registered line magic `run_viz`\n"
     ]
    }
   ],
   "source": [
    "%reload_kedro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "suspended-mongolia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-05 15:44:44,866 - kedro.io.data_catalog - INFO - Loading data from `hp_train` (PickleDataSet)...\n",
      "2021-04-05 15:44:44,955 - kedro.io.data_catalog - INFO - Loading data from `hp_test` (PickleDataSet)...\n",
      "2021-04-05 15:44:44,983 - kedro.io.data_catalog - INFO - Loading data from `hp_valid` (PickleDataSet)...\n",
      "2021-04-05 15:44:45,020 - kedro.io.data_catalog - INFO - Loading data from `hp_y_train` (PickleDataSet)...\n",
      "2021-04-05 15:44:45,046 - kedro.io.data_catalog - INFO - Loading data from `hp_y_test` (PickleDataSet)...\n",
      "2021-04-05 15:44:45,054 - kedro.io.data_catalog - INFO - Loading data from `hp_y_valid` (PickleDataSet)...\n"
     ]
    }
   ],
   "source": [
    "X_train = catalog.load('hp_train')\n",
    "X_test = catalog.load('hp_test')\n",
    "X_valid = catalog.load('hp_valid')\n",
    "y_train = catalog.load('hp_y_train')\n",
    "y_test = catalog.load('hp_y_test')\n",
    "y_valid = catalog.load('hp_y_valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "lasting-liquid",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from typing import Any, Dict\n",
    "from kedro.config import ConfigLoader\n",
    "from hyperopt import fmin, hp, tpe\n",
    "import lightgbm as lgb\n",
    "import mlflow\n",
    "from mlflow import log_metric, log_params\n",
    "from mlflow.lightgbm import log_model\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_log_error\n",
    "from sklearn.metrics import r2_score, median_absolute_error, mean_absolute_percentage_error, mean_absolute_error, mean_squared_error\n",
    "\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "consecutive-craps",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_experiment() -> str:\n",
    "    conf_paths = [\"../conf/local\", \"../conf/base\"]\n",
    "    conf_loader = ConfigLoader(conf_paths=conf_paths)\n",
    "    conf_mlflow = conf_loader.get(\"mlflow.yml\")\n",
    "    experiment_name = conf_mlflow\\\n",
    "        .get(\"experiment\").get(\"name\")\n",
    "    #client = MlflowClient(tracking_uri=conf_mlflow.get(\"mlflow_tracking_uri\"))\n",
    "    client = MlflowClient(\"sqlite:///../data/mlflow.db\")\n",
    "    experiments = client.list_experiments()\n",
    "    lista = list(filter(lambda x: x.name==experiment_name, experiments))\n",
    "    return lista[0].experiment_id if  len(lista)>0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-arizona",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "first-jefferson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-05 16:19:31,897 - alembic.runtime.migration - INFO - Context impl SQLiteImpl.\n",
      "2021-04-05 16:19:31,899 - alembic.runtime.migration - INFO - Will assume non-transactional DDL.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_get_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "round-simon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sqlite:///data/mlflow.db'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_paths = [\"../conf/local\", \"../conf/base\"]\n",
    "conf_loader = ConfigLoader(conf_paths=conf_paths)\n",
    "conf_mlflow = conf_loader.get(\"mlflow.yml\")\n",
    "experiment_name = conf_mlflow\\\n",
    "    .get(\"experiment\").get(\"name\")\n",
    "conf_mlflow.get(\"mlflow_tracking_uri\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "opposite-comparative",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_tracking_uri() -> str:\n",
    "    conf_paths = [\"../conf/local\", \"../conf/base\"]\n",
    "    conf_loader = ConfigLoader(conf_paths=conf_paths)\n",
    "    conf_mlflow = conf_loader.get(\"mlflow.yml\")\n",
    "    experiment_name = conf_mlflow\\\n",
    "        .get(\"experiment\").get(\"name\")\n",
    "    return conf_mlflow.get(\"mlflow_tracking_uri\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "formal-tamil",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sqlite:///data/mlflow.db'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_get_tracking_uri()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pointed-potato",
   "metadata": {},
   "source": [
    "**UWAGA** trzeba odpalić kod w glownym repo inaczej tworzy folder `mlrun` tam gdzie notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "leading-asthma",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _objective(\n",
    "    params: Dict,\n",
    "    X_train: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    y_test: np.ndarray) -> float:\n",
    "    \n",
    "    experiment_id = _get_experiment()\n",
    "    \n",
    "    mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "    mlflow.lightgbm.autolog(log_input_examples=False, \n",
    "                            log_model_signatures=False, \n",
    "                            log_models=True, \n",
    "                            disable=False, \n",
    "                            exclusive=False, \n",
    "                            disable_for_unsupported_versions=False, \n",
    "                            silent=False)\n",
    "    \n",
    "    with mlflow.start_run(experiment_id=experiment_id, nested=True):\n",
    "        params['deterministic'] = True\n",
    "        params['objective'] = \"regression_l2\"\n",
    "        params['boosting'] = \"gbdt\"\n",
    "        params['metric'] = ['l1','mape']\n",
    "        params['seed'] = '666'\n",
    "        \n",
    "        #log_params(params)\n",
    "        \n",
    "        train_params = {\n",
    "            'num_boost_round': 300,\n",
    "            'verbose_eval': False,\n",
    "            'early_stopping_rounds': 10,\n",
    "        }\n",
    "        \n",
    "        train_data = lgb.Dataset(X_train, label=y_train, params={'verbose': -1})\n",
    "        test_data = lgb.Dataset(X_test, label=y_test, params={'verbose': -1})\n",
    "        \n",
    "        #logger.info('Fitting model')\n",
    "\n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            train_data,\n",
    "            valid_sets=[train_data, test_data],\n",
    "            valid_names=['train', 'valid'],\n",
    "            **train_params,\n",
    "        )\n",
    "                \n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_train = model.predict(X_train)\n",
    "\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "        mae = median_absolute_error(y_test, y_pred)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        \n",
    "        r2_tr = r2_score(y_train, y_pred_train)\n",
    "        mape_tr = mean_absolute_percentage_error(y_train, y_pred_train)\n",
    "        mae_tr = median_absolute_error(y_train, y_pred_train)\n",
    "        mse_tr = mean_squared_error(y_train, y_pred_train)\n",
    "        \n",
    "                   \n",
    "        #log_metric(\"mae_tst\", round(mae/1000,1))\n",
    "        #log_metric(\"mae_tra\", round(mae_tr/1000,1))\n",
    "        \n",
    "        #log_metric(\"r2_tst\", r2)\n",
    "        #log_metric(\"r2_tra\", r2_tr)\n",
    "\n",
    "        #log_model(lgb_model=bst, \n",
    "        #          artifact_path=\"model\",\n",
    "        #          registered_model_name=\"LightgbmPricePredictor\",\n",
    "        #         )\n",
    "\n",
    "        \n",
    "        return mae\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "personalized-marking",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hp_tuning(\n",
    "    X_train: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    y_test: np.ndarray,\n",
    "    parameters: Dict,\n",
    "    ):\n",
    "\n",
    "    space = {\n",
    "        \"learning_rate\": hp.uniform(\"learning_rate\", 0.001, 0.5),\n",
    "        \"max_bin\": hp.randint(\"max_bin\", 100, 10000),\n",
    "        \"max_depth\": hp.randint(\"max_depth\", 2, 25),\n",
    "        \"min_data_in_leaf\": hp.randint(\"min_data_in_leaf\", 10, 150),\n",
    "        \"num_leaves\": hp.randint(\"num_leaves\", 10, 300),\n",
    "        \"lambda_l1\": hp.uniform(\"lambda_l1\", 0.0, 0.5),\n",
    "        \"lambda_l2\": hp.uniform(\"lambda_l2\", 0.0, 2.0),\n",
    "        \"bagging_fraction\": hp.uniform(\"bagging_fraction\", 0.3, 1.0), \n",
    "        \"bagging_freq\": hp.randint(\"bagging_freq\", 1, 10),       \n",
    "        \"feature_fraction\": hp.uniform(\"feature_fraction\", 0.3, 1.0)\n",
    "        }\n",
    "    \n",
    "    name = \"LightgbmPricePredictor\"\n",
    "    tags = {\"framework\": \"Lightgbm\"}\n",
    "    desc = \"This price predition model is build for Warsaw flats price assessments.\"\n",
    "\n",
    "    client = MlflowClient(\"sqlite:///../data/mlflow.db\")\n",
    "    \n",
    "    try:\n",
    "        client.create_registered_model(name, tags, desc)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    best = fmin(\n",
    "        partial(_objective, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test), \n",
    "        space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=parameters[\"hp_number_of_experiments\"])\n",
    "    return best\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "designed-promise",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-05 15:44:45,937 - alembic.runtime.migration - INFO - Context impl SQLiteImpl.\n",
      "2021-04-05 15:44:45,937 - alembic.runtime.migration - INFO - Will assume non-transactional DDL.\n",
      "  0%|          | 0/2 [00:00<?, ?trial/s, best loss=?]2021-04-05 15:44:45,987 - hyperopt.tpe - INFO - build_posterior_wrapper took 0.002698 seconds\n",
      "2021-04-05 15:44:45,988 - hyperopt.tpe - INFO - TPE using 0 trials\n",
      "2021-04-05 15:44:46,044 - alembic.runtime.migration - INFO - Context impl SQLiteImpl.\n",
      "2021-04-05 15:44:46,045 - alembic.runtime.migration - INFO - Will assume non-transactional DDL.\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.889608 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 880229                  \n",
      "[LightGBM] [Info] Number of data points in the train set: 30000, number of used features: 1542\n",
      " 50%|█████     | 1/2 [01:20<01:20, 80.43s/trial, best loss: 42566.78755875409]2021-04-05 15:46:06,428 - hyperopt.tpe - INFO - build_posterior_wrapper took 0.005325 seconds\n",
      "2021-04-05 15:46:06,432 - hyperopt.tpe - INFO - TPE using 1/1 trials with best loss 42566.787559\n",
      "2021-04-05 15:46:06,461 - alembic.runtime.migration - INFO - Context impl SQLiteImpl.\n",
      "2021-04-05 15:46:06,463 - alembic.runtime.migration - INFO - Will assume non-transactional DDL.\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.513485 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 943301                                           \n",
      "[LightGBM] [Info] Number of data points in the train set: 30000, number of used features: 1205\n",
      "100%|██████████| 2/2 [02:38<00:00, 79.29s/trial, best loss: 39661.447623945365]\n"
     ]
    }
   ],
   "source": [
    "results = hp_tuning(\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    y_train=y_train,\n",
    "    y_test=y_test,\n",
    "    parameters={\"hp_number_of_experiments\": 2},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ranging-accuracy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bagging_fraction': 0.649225804927887,\n",
       " 'bagging_freq': 4,\n",
       " 'feature_fraction': 0.6729394484805863,\n",
       " 'lambda_l1': 0.2234498923931597,\n",
       " 'lambda_l2': 0.8964440633850108,\n",
       " 'learning_rate': 0.30032452983063834,\n",
       " 'max_bin': 7572,\n",
       " 'max_depth': 4,\n",
       " 'min_data_in_leaf': 110,\n",
       " 'num_leaves': 259}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "romance-blanket",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bagging_fraction': 0.64922583,\n",
       " 'bagging_freq': 4,\n",
       " 'feature_fraction': 0.6729394,\n",
       " 'lambda_l1': 0.22344989,\n",
       " 'lambda_l2': 0.8964441,\n",
       " 'learning_rate': 0.30032453,\n",
       " 'max_bin': 7572,\n",
       " 'max_depth': 4,\n",
       " 'min_data_in_leaf': 110,\n",
       " 'num_leaves': 259}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for k in results:\n",
    "    if type(results[k]) == np.int64:\n",
    "        results[k] = results[k].astype(np.int32)\n",
    "    elif type(results[k]) == np.float64:\n",
    "        results[k] = results[k].astype(np.float32)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "worth-findings",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type int64 is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-a2b8e27cd7d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/kedro/lib/python3.8/json/__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mindent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mseparators\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         default is None and not sort_keys and not kw):\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kedro/lib/python3.8/json/encoder.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# exceptions aren't as detailed.  The list call should be roughly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kedro/lib/python3.8/json/encoder.py\u001b[0m in \u001b[0;36miterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
      "\u001b[0;32m~/miniconda3/envs/kedro/lib/python3.8/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[0;32m--> 179\u001b[0;31m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[1;32m    180\u001b[0m                         f'is not JSON serializable')\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type int64 is not JSON serializable"
     ]
    }
   ],
   "source": [
    "import json\n",
    "json.dumps(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xflats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
